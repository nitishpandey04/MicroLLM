[project]
name = "microllm"
version = "0.1.0"
description = "small language models research"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "datasets>=4.4.1",
    "ipykernel>=7.0.1",
    "numpy>=2.2.6",
    "tiktoken>=0.12.0",
    "tokenizers>=0.22.1",
    "torch>=2.9.0",
]

[build-system]
requires = ["maturin>=1.7,<2.0"]
build-backend = "maturin"

[tool.maturin]
module-name = "rustbpe"
bindings = "pyo3"
python-source = "."
manifest-path = "rustbpe/Cargo.toml"

[dependency-groups]
dev = [
    "maturin>=1.9.4",
    "pytest>=8.0.0",
]

[tool.pytest.ini_options]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]

# target torch to cuda 12.8 or CPU
[tool.uv.sources]
torch = [  
    { index = "pytorch-cpu", extra = "cpu" },  
    { index = "pytorch-cu128", extra = "gpu" },  
]

[[tool.uv.index]]  
name = "pytorch-cpu"  
url = "https://download.pytorch.org/whl/cpu"  
explicit = true  
  
[[tool.uv.index]]  
name = "pytorch-cu128"  
url = "https://download.pytorch.org/whl/cu128"  
explicit = true

[project.optional-dependencies]  
cpu = [  
    "torch>=2.8.0",  
]  
gpu = [  
    "torch>=2.8.0",  
]  
  
[tool.uv]  
conflicts = [  
    [  
        { extra = "cpu" },  
        { extra = "gpu" },  
    ],  
]  
